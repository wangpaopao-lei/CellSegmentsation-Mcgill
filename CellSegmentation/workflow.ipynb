{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import spateo as st\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from scipy.sparse import lil_matrix, csr_matrix, vstack"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 Read 2 files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m bin_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/Mouse_brain_Adult_GEM_bin1_sub.tsv\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      3\u001B[0m image_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/Mouse_brain_Adult_sub.tif\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 4\u001B[0m adatasub \u001B[38;5;241m=\u001B[39m \u001B[43mst\u001B[49m\u001B[38;5;241m.\u001B[39mio\u001B[38;5;241m.\u001B[39mread_bgi_agg(bin_file, image_file)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'st' is not defined"
     ]
    }
   ],
   "source": [
    "# read file\n",
    "bin_file = 'data/Mouse_brain_Adult_GEM_bin1_sub.tsv'\n",
    "image_file = 'data/Mouse_brain_Adult_sub.tif'\n",
    "adatasub = st.io.read_bgi_agg(bin_file, image_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T20:16:44.243183Z",
     "start_time": "2023-09-27T20:16:44.241219Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bin_size=3\n",
    "n_neighbor=50\n",
    "r_estimate=15\n",
    "startx='0'\n",
    "starty='0'\n",
    "patchsize='0'\n",
    "\n",
    "adatasub.layers['unspliced'] = adatasub.X\n",
    "patchsizex = adatasub.X.shape[0]\n",
    "patchsizey = adatasub.X.shape[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Run Watershed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# nucleus segmentation from staining image\n",
    "fig, ax = plt.subplots(figsize=(8, 8), tight_layout=True)\n",
    "st.cs.mask_nuclei_from_stain(adatasub, otsu_classes=4, otsu_index=1)\n",
    "st.pl.imshow(adatasub, 'stain_mask', ax=ax)\n",
    "st.cs.find_peaks_from_mask(adatasub, 'stain', 7)\n",
    "st.cs.watershed(adatasub, 'stain', 5, out_layer='watershed_labels')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), tight_layout=True)\n",
    "st.pl.imshow(adatasub, 'stain', save_show_or_return='return', ax=ax)\n",
    "st.pl.imshow(adatasub, 'watershed_labels', labels=True, alpha=0.5, ax=ax)\n",
    "plt.savefig('fig/watershed_labels' + startx + ':' + starty + ':' + '.png')\n",
    "# adatasub.write('data/Mouse_brain_Adult_5800:8000:900:900.h5ad')\n",
    "# print(adatasub)\n",
    "\n",
    "adatasub.write('data/spots' + startx + ':' + starty + ':' + '.h5ad')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Real-value output(prob_map)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def watershed_with_probability(adatasub, stain_layer='stain', peak_distance=7, num_iterations=100):\n",
    "    # Initial steps\n",
    "    st.cs.mask_nuclei_from_stain(adatasub, otsu_classes=4, otsu_index=1)\n",
    "    st.cs.find_peaks_from_mask(adatasub, stain_layer, peak_distance)\n",
    "\n",
    "    markers = adatasub.obsm['peaks']\n",
    "    gradient = filters.sobel(adatasub.X)\n",
    "\n",
    "    # Run Watershed once to get the set of possible labels\n",
    "    initial_segmentation = segmentation.watershed(gradient, markers=markers)\n",
    "    unique_labels = np.unique(initial_segmentation)\n",
    "\n",
    "    # Create a frequency map for each label\n",
    "    freq_maps = {label: np.zeros_like(gradient, dtype=float) for label in unique_labels}\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        perturbed_markers = markers + np.random.normal(scale=1, size=markers.shape)\n",
    "        perturbed_markers = np.clip(perturbed_markers, 0, min(gradient.shape)-1)\n",
    "\n",
    "        segmented = segmentation.watershed(gradient, markers=perturbed_markers)\n",
    "\n",
    "        # Update the frequency maps\n",
    "        for label in unique_labels:\n",
    "            freq_maps[label] += (segmented == label)\n",
    "\n",
    "    # Normalize each frequency map\n",
    "    for label in unique_labels:\n",
    "        freq_maps[label] /= num_iterations\n",
    "\n",
    "    return freq_maps\n",
    "\n",
    "# This will return a dictionary of frequency maps, one for each label.\n",
    "freq_maps_dict = watershed_with_probability(adatasub)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Prepare data for Model1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "watershed2x = {}\n",
    "watershed2y = {}\n",
    "for i in range(adatasub.layers['watershed_labels'].shape[0]):\n",
    "    for j in range(adatasub.layers['watershed_labels'].shape[1]):\n",
    "        if adatasub.layers['watershed_labels'][i, j] == 0:\n",
    "            continue\n",
    "        if adatasub.layers['watershed_labels'][i, j] in watershed2x:\n",
    "            watershed2x[adatasub.layers['watershed_labels'][i, j]].append(i)\n",
    "            watershed2y[adatasub.layers['watershed_labels'][i, j]].append(j)\n",
    "        else:\n",
    "            watershed2x[adatasub.layers['watershed_labels'][i, j]] = [i]\n",
    "            watershed2y[adatasub.layers['watershed_labels'][i, j]] = [j]\n",
    "\n",
    "watershed2center = {}\n",
    "sizes = []\n",
    "for nucleus in watershed2x:\n",
    "    watershed2center[nucleus] = [np.mean(watershed2x[nucleus]), np.mean(watershed2y[nucleus])]\n",
    "    sizes.append(len(watershed2x[nucleus]))\n",
    "# print(np.min(sizes), np.max(sizes), np.mean(sizes))\n",
    "# print('#nucleus', len(watershed2center))\n",
    "# find xmin ymin\n",
    "xall = []\n",
    "yall = []\n",
    "with open(bin_file) as fr:\n",
    "    header = fr.readline()\n",
    "    for line in fr:\n",
    "        gene, x, y, count = line.split()\n",
    "        xall.append(int(x))\n",
    "        yall.append(int(y))\n",
    "xmin = np.min(xall)\n",
    "ymin = np.min(yall)\n",
    "# print(np.min(xall), np.min(yall), np.max(xall), np.max(yall))\n",
    "\n",
    "# find all the genes in the range\n",
    "geneid = {}\n",
    "genecnt = 0\n",
    "id2gene = {}\n",
    "with open(bin_file) as fr:\n",
    "    header = fr.readline()\n",
    "    for line in fr:\n",
    "        gene, x, y, count = line.split()\n",
    "        if gene not in geneid:\n",
    "            geneid[gene] = genecnt\n",
    "            id2gene[genecnt] = gene\n",
    "            genecnt += 1\n",
    "\n",
    "idx2exp = {}\n",
    "downrs = bin_size\n",
    "with open(bin_file) as fr:\n",
    "    header = fr.readline()\n",
    "    for line in fr:\n",
    "        gene, x, y, count = line.split()\n",
    "        x = int(x) - xmin\n",
    "        y = int(y) - ymin\n",
    "        if gene not in geneid:\n",
    "            continue\n",
    "        if int(x) < int(startx) or int(x) >= int(startx) + int(patchsizex) or int(y) < int(starty) or int(y) >= int(\n",
    "                starty) + int(patchsizey):\n",
    "            continue\n",
    "        idx = int(math.floor((int(x) - int(startx)) / downrs) * math.ceil(patchsizey / downrs) + math.floor(\n",
    "            (int(y) - int(starty)) / downrs))\n",
    "        if idx not in idx2exp:\n",
    "            idx2exp[idx] = {}\n",
    "            idx2exp[idx][geneid[gene]] = int(count)\n",
    "        elif geneid[gene] not in idx2exp[idx]:\n",
    "            idx2exp[idx][geneid[gene]] = int(count)\n",
    "        else:\n",
    "            idx2exp[idx][geneid[gene]] += int(count)\n",
    "\n",
    "all_exp_merged_bins = lil_matrix((int(math.ceil(patchsizex / downrs) * math.ceil(patchsizey / downrs)), genecnt),\n",
    "                                 dtype=np.int8)\n",
    "for idx in idx2exp:\n",
    "    for gid in idx2exp[idx]:\n",
    "        all_exp_merged_bins[idx, gid] = idx2exp[idx][gid]\n",
    "        # print(idx, gid, idx2exp[idx][gid])\n",
    "all_exp_merged_bins = all_exp_merged_bins.tocsr()\n",
    "# print(all_exp_merged_bins.shape)\n",
    "\n",
    "all_exp_merged_bins_ad = ad.AnnData(\n",
    "    all_exp_merged_bins,\n",
    "    obs=pd.DataFrame(index=[i for i in range(all_exp_merged_bins.shape[0])]),\n",
    "    var=pd.DataFrame(index=[i for i in range(all_exp_merged_bins.shape[1])]),\n",
    ")\n",
    "sc.pp.highly_variable_genes(all_exp_merged_bins_ad, n_top_genes=2000, flavor='seurat_v3', span=1.0)\n",
    "selected_index = all_exp_merged_bins_ad.var[all_exp_merged_bins_ad.var.highly_variable].index\n",
    "selected_index = list(selected_index)\n",
    "selected_index = [int(i) for i in selected_index]\n",
    "with open('data/variable_genes' + startx + ':' + starty + ':' + patchsize + ':' + patchsize + '.txt', 'w') as fw:\n",
    "    for id in selected_index:\n",
    "        fw.write(id2gene[id] + '\\n')\n",
    "\n",
    "# check total gene counts\n",
    "all_exp_merged_bins = all_exp_merged_bins.toarray()[:, selected_index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "distance_ratio_threshold = 0.3\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# 前面已经定义的变量\n",
    "# downrs, offsets, all_exp_merged_bins, adatasub, n_neighbor, patchsizey\n",
    "\n",
    "# 初始化变量\n",
    "x_test_tmp = []\n",
    "x_test = []\n",
    "x_test_pos = []\n",
    "\n",
    "x_optimize_train = []\n",
    "x_optimize_pos = []\n",
    "x_optimize_labels = []\n",
    "y_optimize_train = []\n",
    "offsets = []\n",
    "for dis in range(1, 11):\n",
    "    for dy in range(-dis, dis + 1):\n",
    "        offsets.append([-dis * downrs, dy * downrs])\n",
    "    for dy in range(-dis, dis + 1):\n",
    "        offsets.append([dis * downrs, dy * downrs])\n",
    "    for dx in range(-dis + 1, dis):\n",
    "        offsets.append([dx * downrs, -dis * downrs])\n",
    "    for dx in range(-dis + 1, dis):\n",
    "        offsets.append([dx * downrs, dis * downrs])\n",
    "for i in range(adatasub.layers['watershed_labels'].shape[0]):\n",
    "    for j in range(adatasub.layers['watershed_labels'].shape[1]):\n",
    "        if (not i % downrs == 0) or (not j % downrs == 0):\n",
    "            continue\n",
    "        idx = int(math.floor(i / downrs) * math.ceil(patchsizey / downrs) + math.floor(j / downrs))\n",
    "        label = adatasub.layers['watershed_labels'][i, j]\n",
    "\n",
    "        # For Optimize_train\n",
    "        if label > 0:\n",
    "            x_optimize_sample = [all_exp_merged_bins[idx, :]]\n",
    "            x_optimize_pos_sample = [[i, j]]\n",
    "            y_optimize_sample = [watershed2center[label]]\n",
    "            x_optimize_labels_sample = [label]\n",
    "            for dx, dy in offsets:\n",
    "                if len(x_optimize_sample) == n_neighbor:\n",
    "                    break\n",
    "                x = i + dx\n",
    "                y = j + dy\n",
    "                if 0 <= x < adatasub.layers['watershed_labels'].shape[0] and 0 <= y < adatasub.layers['watershed_labels'].shape[1]:\n",
    "                    idx_nb = int(math.floor(x / downrs) * math.ceil(patchsizey / downrs) + math.floor(y / downrs))\n",
    "                    if 0 <= idx_nb < all_exp_merged_bins.shape[0] and np.sum(all_exp_merged_bins[idx_nb, :]) > 0:\n",
    "                        x_optimize_sample.append(all_exp_merged_bins[idx_nb, :])\n",
    "                        x_optimize_pos_sample.append([x, y])\n",
    "                        x_optimize_labels_sample.append(adatasub.layers['watershed_labels'][x, y])\n",
    "                        y_optimize_sample.append(watershed2center.get(adatasub.layers['watershed_labels'][x, y], [-1, -1]))\n",
    "            if len(x_optimize_sample) < n_neighbor:\n",
    "                continue\n",
    "            x_optimize_train.append(x_optimize_sample)\n",
    "            x_optimize_pos.append(x_optimize_pos_sample)\n",
    "            x_optimize_labels.append(x_optimize_labels_sample)\n",
    "            y_optimize_train.append(y_optimize_sample)\n",
    "\n",
    "        # For x_test\n",
    "        elif label == 0:\n",
    "            backgroud = True\n",
    "            for nucleus in watershed2center:\n",
    "                if (i - watershed2center[nucleus][0]) ** 2 + (j - watershed2center[nucleus][1]) ** 2 <= 900 or \\\n",
    "                        adatasub.layers['stain'][i, j] > 10:\n",
    "                    backgroud = False\n",
    "                    break\n",
    "            if backgroud:\n",
    "                continue  # Skip background\n",
    "\n",
    "            x_test_sample = [all_exp_merged_bins[idx, :]]\n",
    "            x_test_pos_sample = [[i, j]]\n",
    "            for dx, dy in offsets:\n",
    "                if len(x_test_sample) == n_neighbor:\n",
    "                    break\n",
    "                x = i + dx\n",
    "                y = j + dy\n",
    "                if 0 <= x < adatasub.layers['watershed_labels'].shape[0] and 0 <= y < adatasub.layers['watershed_labels'].shape[1]:\n",
    "                    idx_nb = int(math.floor(x / downrs) * math.ceil(patchsizey / downrs) + math.floor(y / downrs))\n",
    "                    if 0 <= idx_nb < all_exp_merged_bins.shape[0] and np.sum(all_exp_merged_bins[idx_nb, :]) > 0:\n",
    "                        x_test_sample.append(all_exp_merged_bins[idx_nb, :])\n",
    "                        x_test_pos_sample.append([x, y])\n",
    "            if len(x_test_sample) < n_neighbor:\n",
    "                continue\n",
    "            x_test_tmp.append(x_test_sample)\n",
    "            if len(x_test_tmp) > 500:\n",
    "                x_test.extend(x_test_tmp)\n",
    "                x_test_tmp = []\n",
    "            x_test_pos.append(x_test_pos_sample)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 Save Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 结束循环后\n",
    "x_test.extend(x_test_tmp)\n",
    "x_test = np.array(x_test)\n",
    "x_test_pos = np.array(x_test_pos)\n",
    "\n",
    "x_optimize_train = np.array(x_optimize_train)\n",
    "x_optimize_pos = np.array(x_optimize_pos)\n",
    "x_optimize_labels = np.array(x_optimize_labels)\n",
    "y_optimize_train = np.array(y_optimize_train)\n",
    "\n",
    "# 保存到文件\n",
    "np.savez_compressed('data/x_test.npz',x_test=x_test)\n",
    "np.savez_compressed('data/x_test.npz',x_test=x_test)\n",
    "np.savez_compressed('data/x_optimize_train.npz', x_optimize_train=x_optimize_train)\n",
    "np.savez_compressed('data/x_optimize_pos.npz', x_optimize_pos=x_optimize_pos)\n",
    "np.savez_compressed('data/x_optimize_labels.npz', x_optimize_labels=x_optimize_labels)\n",
    "np.savez_compressed('data/y_optimize_train.npz', y_optimize_train=y_optimize_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5 util"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dir_to_class(y_dir, class_num):\n",
    "    y_dir_class = []\n",
    "    for i in range(len(y_dir)):\n",
    "        x, y = y_dir[i]\n",
    "        if x == -9999:\n",
    "            y_vec = np.zeros(class_num)\n",
    "            y_dir_class.append(y_vec)\n",
    "        else:\n",
    "            if y == 0 and x > 0:\n",
    "                deg = np.arctan(float('inf'))\n",
    "            elif y == 0 and x < 0:\n",
    "                deg = np.arctan(-float('inf'))\n",
    "            elif y == 0 and x == 0:\n",
    "                deg = np.arctan(0)\n",
    "            else:\n",
    "                deg = np.arctan((x / y))\n",
    "            if (x > 0 and y < 0) or (x <= 0 and y < 0):\n",
    "                deg += np.pi\n",
    "            elif x < 0 and y >= 0:\n",
    "                deg += 2 * np.pi\n",
    "            cla = int(deg / (2 * np.pi / class_num))\n",
    "            y_vec = np.zeros(class_num)\n",
    "            y_vec[cla] = 1\n",
    "            y_dir_class.append(y_vec)\n",
    "    return np.array(y_dir_class)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transformer to correct Watershed\n",
    "class Model1(nn.Module):\n",
    "    def __init__(self,gene_dim,dir_dim,hidden_dim,label_dim):\n",
    "        super(Model1,self).__init__()\n",
    "\n",
    "        self.dense_gene = nn.Linear(gene_dim, hidden_dim)\n",
    "        self.dense_dir = nn.Linear(dir_dim, hidden_dim)\n",
    "        # watershed label is a number\n",
    "        self.dense_label = nn.Linear(label_dim, hidden_dim)\n",
    "# ToDo(1):设计transformer模型具体结构\n",
    "        self.transformer=nn.Transformer(hidden_dim,1)\n",
    "\n",
    "    def forward(self,gene_data,dir_data,label_data):\n",
    "        gene_data=self.dense_gene(gene_data)\n",
    "        dir_data=self.dense_dir(dir_data)\n",
    "        label_data=self.dense_label(label_data)\n",
    "\n",
    "        fused_data=gene_data+dir_data+label_data\n",
    "\n",
    "        out=self.transformer(fused_data)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Model1 training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model=Model1()\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "# ToDo(2):训练流程需要细化，包含初始化-迭代-结束三个部分\n",
    "for i in range(epochs):\n",
    "    for trains,labels in enumerate(train_loader):\n",
    "            outputs = model(trains)\n",
    "            model.zero_grad()\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5 Postprocess and prepare data for model2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
