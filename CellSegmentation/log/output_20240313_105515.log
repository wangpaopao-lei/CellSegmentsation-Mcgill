2024-03-13 10:55:17.855036: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lei/miniconda3/envs/CS/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-12.1/lib64:
2024-03-13 10:55:17.855086: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/lei/miniconda3/envs/CS/lib/python3.9/site-packages/spaghetti/network.py:40: FutureWarning:

The next major release of pysal/spaghetti (2.0.0) will drop support for all ``libpysal.cg`` geometries. This change is a first step in refactoring ``spaghetti`` that is expected to result in dramatically reduced runtimes for network instantiation and operations. Users currently requiring network and point pattern input as ``libpysal.cg`` geometries should prepare for this simply by converting to ``shapely`` geometries.

2024-03-13 10:55:21.069590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-03-13 10:55:21.069965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-03-13 10:55:21.070249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-03-13 10:55:21.070543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-03-13 10:55:21.070823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-03-13 10:55:21.071262: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lei/miniconda3/envs/CS/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-12.1/lib64:
2024-03-13 10:55:21.071349: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lei/miniconda3/envs/CS/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-12.1/lib64:
2024-03-13 10:55:21.071437: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lei/miniconda3/envs/CS/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-12.1/lib64:
2024-03-13 10:55:21.071518: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lei/miniconda3/envs/CS/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-12.1/lib64:
2024-03-13 10:55:21.072279: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lei/miniconda3/envs/CS/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-12.1/lib64:
2024-03-13 10:55:21.072323: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2024-03-13 10:55:21.072573: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/lei/miniconda3/envs/CS/lib/python3.9/site-packages/ot/backend.py:2998: UserWarning:

To use TensorflowBackend, you need to activate the tensorflow numpy API. You can activate it by running: 
from tensorflow.python.ops.numpy_ops import np_config
np_config.enable_numpy_behavior()

|-----> Constructing count matrices.
|-----> <insert> __type to uns in AnnData Object.
|-----> <insert> pp to uns in AnnData Object.
|-----> <insert> spatial to uns in AnnData Object.
|-----> <select> stain layer in AnnData Object
|-----> Constructing nuclei mask from staining image.
|-----> <insert> stain_mask to layers in AnnData Object.
|-----> <select> stain_mask layer in AnnData Object
|-----> Finding peaks with minimum distance 7.
|-----> <insert> stain_distances to layers in AnnData Object.
|-----> <insert> stain_markers to layers in AnnData Object.
|-----> <select> stain layer in AnnData Object
|-----> <select> stain_mask layer in AnnData Object
|-----> <select> stain_markers layer in AnnData Object
|-----> Running Watershed.
|-----> <insert> watershed_labels to layers in AnnData Object.
|-----> <select> watershed_labels layer in AnnData Object
watershed labels prepared.
/home/lei/miniconda3/envs/CS/lib/python3.9/site-packages/anndata/_core/anndata.py:119: ImplicitModificationWarning:

Transforming to str index.

h5ad saved!
(1200, 1200, 2000)
gene data prepared.
batch folder exists
finished 8%
finished 17%
finished 25%
finished 33%
finished 42%
finished 50%
finished 58%
finished 67%
finished 75%
finished 83%
finished 92%
finished 100%
Core samples: 16952, Outer samples: 320161
done!
preprocess done!
2024-03-13 11:48:19.758840: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.1/lib64:
2024-03-13 11:48:19.758885: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
results folder exists.
2024-03-13 11:48:53.523774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-03-13 11:48:53.524153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-03-13 11:48:53.634985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-03-13 11:48:53.635333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-03-13 11:48:53.635676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-03-13 11:48:53.636070: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.1/lib64:
2024-03-13 11:48:53.636164: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.1/lib64:
2024-03-13 11:48:53.636257: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.1/lib64:
2024-03-13 11:48:53.636339: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.1/lib64:
2024-03-13 11:48:53.664735: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.1/lib64:
2024-03-13 11:48:53.664839: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.1/lib64:
2024-03-13 11:48:53.664855: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2024-03-13 11:48:53.665092: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/30
Epoch 1: Actual Labels: [0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0]
Epoch 1: Predictions:
 [[0.4296044 ]
 [0.5705084 ]
 [0.57838386]
 [0.53577894]
 [0.45255324]
 [0.46721894]
 [0.4642988 ]
 [0.42890978]
 [0.44787467]
 [0.49191475]
 [0.45800152]
 [0.46349052]
 [0.43956906]
 [0.46794304]
 [0.53849643]
 [0.49118412]
 [0.59297603]
 [0.43306163]
 [0.6013603 ]
 [0.57509667]
 [0.45655417]
 [0.45216158]
 [0.4382547 ]
 [0.41755566]
 [0.547716  ]
 [0.547716  ]
 [0.5122482 ]
 [0.5882027 ]
 [0.49664256]
 [0.441236  ]
 [0.62940514]
 [0.45388678]]
679/679 - 19s - loss: 0.6983 - accuracy: 0.5292 - val_loss: 0.6857 - val_accuracy: 0.5563 - 19s/epoch - 29ms/step
Epoch 2/30
679/679 - 16s - loss: 0.6832 - accuracy: 0.5537 - val_loss: 0.6783 - val_accuracy: 0.5701 - 16s/epoch - 24ms/step
Epoch 3/30
679/679 - 17s - loss: 0.6760 - accuracy: 0.5665 - val_loss: 0.6736 - val_accuracy: 0.5812 - 17s/epoch - 25ms/step
Epoch 4/30
679/679 - 17s - loss: 0.6641 - accuracy: 0.5837 - val_loss: 0.6632 - val_accuracy: 0.5766 - 17s/epoch - 25ms/step
Epoch 5/30
679/679 - 17s - loss: 0.6583 - accuracy: 0.5861 - val_loss: 0.6586 - val_accuracy: 0.5869 - 17s/epoch - 25ms/step
Epoch 6/30
679/679 - 17s - loss: 0.6487 - accuracy: 0.5996 - val_loss: 0.6364 - val_accuracy: 0.6197 - 17s/epoch - 25ms/step
Epoch 7/30
679/679 - 17s - loss: 0.4325 - accuracy: 0.7680 - val_loss: 0.5075 - val_accuracy: 0.8323 - 17s/epoch - 25ms/step
Epoch 8/30
679/679 - 16s - loss: 0.0769 - accuracy: 0.9773 - val_loss: 0.0139 - val_accuracy: 0.9998 - 16s/epoch - 24ms/step
Epoch 9/30
679/679 - 16s - loss: 0.0421 - accuracy: 0.9895 - val_loss: 0.1170 - val_accuracy: 0.9875 - 16s/epoch - 24ms/step
Epoch 10/30
679/679 - 17s - loss: 0.0545 - accuracy: 0.9826 - val_loss: 0.0056 - val_accuracy: 0.9989 - 17s/epoch - 24ms/step
Epoch 11/30
679/679 - 16s - loss: 0.0187 - accuracy: 0.9931 - val_loss: 0.0018 - val_accuracy: 0.9998 - 16s/epoch - 24ms/step
Epoch 12/30
679/679 - 16s - loss: 0.0023 - accuracy: 0.9996 - val_loss: 1.7632e-04 - val_accuracy: 1.0000 - 16s/epoch - 24ms/step
Epoch 13/30
679/679 - 16s - loss: 0.0665 - accuracy: 0.9793 - val_loss: 9.6421e-04 - val_accuracy: 1.0000 - 16s/epoch - 24ms/step
Epoch 14/30
679/679 - 17s - loss: 0.0041 - accuracy: 0.9989 - val_loss: 1.5053e-04 - val_accuracy: 1.0000 - 17s/epoch - 25ms/step
Epoch 15/30
679/679 - 17s - loss: 0.0596 - accuracy: 0.9785 - val_loss: 0.0257 - val_accuracy: 0.9895 - 17s/epoch - 24ms/step
Epoch 16/30
679/679 - 16s - loss: 0.0079 - accuracy: 0.9977 - val_loss: 1.1972e-04 - val_accuracy: 1.0000 - 16s/epoch - 24ms/step
Epoch 17/30
679/679 - 17s - loss: 1.2775e-04 - accuracy: 1.0000 - val_loss: 1.4312e-04 - val_accuracy: 1.0000 - 17s/epoch - 25ms/step
Epoch 18/30
679/679 - 17s - loss: 0.0572 - accuracy: 0.9857 - val_loss: 0.0044 - val_accuracy: 1.0000 - 17s/epoch - 25ms/step
Epoch 19/30
679/679 - 17s - loss: 0.0040 - accuracy: 0.9988 - val_loss: 2.2536e-04 - val_accuracy: 1.0000 - 17s/epoch - 25ms/step
Epoch 20/30
679/679 - 16s - loss: 0.0199 - accuracy: 0.9938 - val_loss: 0.0010 - val_accuracy: 0.9998 - 16s/epoch - 24ms/step
Epoch 21/30
679/679 - 16s - loss: 0.0847 - accuracy: 0.9746 - val_loss: 6.2583e-04 - val_accuracy: 1.0000 - 16s/epoch - 24ms/step
Epoch 22/30
679/679 - 16s - loss: 0.0456 - accuracy: 0.9970 - val_loss: 3.9933 - val_accuracy: 0.5054 - 16s/epoch - 23ms/step
Epoch 23/30
679/679 - 16s - loss: 0.2908 - accuracy: 0.8744 - val_loss: 0.0765 - val_accuracy: 0.9615 - 16s/epoch - 24ms/step
Epoch 24/30
679/679 - 17s - loss: 0.0258 - accuracy: 0.9916 - val_loss: 2.1556e-04 - val_accuracy: 1.0000 - 17s/epoch - 25ms/step
Epoch 25/30
679/679 - 17s - loss: 0.0244 - accuracy: 0.9940 - val_loss: 1.0046 - val_accuracy: 0.8352 - 17s/epoch - 26ms/step
Epoch 26/30
679/679 - 17s - loss: 0.0482 - accuracy: 0.9863 - val_loss: 3.3109e-04 - val_accuracy: 1.0000 - 17s/epoch - 25ms/step
Epoch 27/30
679/679 - 17s - loss: 2.6831e-04 - accuracy: 1.0000 - val_loss: 1.1640e-04 - val_accuracy: 1.0000 - 17s/epoch - 25ms/step
Epoch 28/30
679/679 - 18s - loss: 0.0927 - accuracy: 0.9684 - val_loss: 0.0226 - val_accuracy: 0.9987 - 18s/epoch - 26ms/step
Epoch 29/30
679/679 - 17s - loss: 0.0844 - accuracy: 0.9798 - val_loss: 0.0119 - val_accuracy: 0.9987 - 17s/epoch - 26ms/step
Epoch 30/30
679/679 - 17s - loss: 0.0887 - accuracy: 0.9693 - val_loss: 0.0099 - val_accuracy: 0.9994 - 17s/epoch - 26ms/step
Inference on all the spots...
144.0 in total
Batch 0 processing...
Batch 0 completed. Time taken: 00:00:18
Batch 1 processing...
Batch 1 completed. Time taken: 00:00:17
Batch 2 processing...
Batch 2 completed. Time taken: 00:00:17
Batch 3 processing...
Batch 3 completed. Time taken: 00:00:17
Batch 4 processing...
Batch 4 completed. Time taken: 00:00:17
Batch 5 processing...
Batch 5 completed. Time taken: 00:00:17
Batch 6 processing...
Batch 6 completed. Time taken: 00:00:17
Batch 7 processing...
Batch 7 completed. Time taken: 00:00:17
Batch 8 processing...
Batch 8 completed. Time taken: 00:00:17
Batch 9 processing...
Batch 9 completed. Time taken: 00:00:17
Batch 10 processing...
Batch 10 completed. Time taken: 00:00:17
Batch 11 processing...
Batch 11 completed. Time taken: 00:00:17
Batch 12 processing...
Batch 12 completed. Time taken: 00:00:17
Batch 13 processing...
Batch 13 completed. Time taken: 00:00:16
Batch 14 processing...
Batch 14 completed. Time taken: 00:00:17
Batch 15 processing...
Batch 15 completed. Time taken: 00:00:16
Batch 16 processing...
Batch 16 completed. Time taken: 00:00:17
Batch 17 processing...
Batch 17 completed. Time taken: 00:00:17
Batch 18 processing...
Batch 18 completed. Time taken: 00:00:17
Batch 19 processing...
Batch 19 completed. Time taken: 00:00:17
Batch 20 processing...
Batch 20 completed. Time taken: 00:00:16
Batch 21 processing...
Batch 21 completed. Time taken: 00:00:16
Batch 22 processing...
Batch 22 completed. Time taken: 00:00:16
Batch 23 processing...
Batch 23 completed. Time taken: 00:00:16
Batch 24 processing...
Batch 24 completed. Time taken: 00:00:16
Batch 25 processing...
Batch 25 completed. Time taken: 00:00:16
Batch 26 processing...
Batch 26 completed. Time taken: 00:00:17
Batch 27 processing...
Batch 27 completed. Time taken: 00:00:16
Batch 28 processing...
Batch 28 completed. Time taken: 00:00:16
Batch 29 processing...
Batch 29 completed. Time taken: 00:00:16
Batch 30 processing...
Batch 30 completed. Time taken: 00:00:16
Batch 31 processing...
Batch 31 completed. Time taken: 00:00:17
Batch 32 processing...
Batch 32 completed. Time taken: 00:00:17
Batch 33 processing...
Batch 33 completed. Time taken: 00:00:17
Batch 34 processing...
Batch 34 completed. Time taken: 00:00:17
Batch 35 processing...
Batch 35 completed. Time taken: 00:00:16
Batch 36 processing...
Batch 36 completed. Time taken: 00:00:16
Batch 37 processing...
Batch 37 completed. Time taken: 00:00:16
Batch 38 processing...
Batch 38 completed. Time taken: 00:00:16
Batch 39 processing...
Batch 39 completed. Time taken: 00:00:16
Batch 40 processing...
Batch 40 completed. Time taken: 00:00:17
Batch 41 processing...
Batch 41 completed. Time taken: 00:00:17
Batch 42 processing...
Batch 42 completed. Time taken: 00:00:16
Batch 43 processing...
Batch 43 completed. Time taken: 00:00:16
Batch 44 processing...
Batch 44 completed. Time taken: 00:00:16
Batch 45 processing...
Batch 45 completed. Time taken: 00:00:16
Batch 46 processing...
Batch 46 completed. Time taken: 00:00:16
Batch 47 processing...
Batch 47 completed. Time taken: 00:00:16
Batch 48 processing...
Batch 48 completed. Time taken: 00:00:16
Batch 49 processing...
Batch 49 completed. Time taken: 00:00:16
Batch 50 processing...
Batch 50 completed. Time taken: 00:00:16
Batch 51 processing...
Batch 51 completed. Time taken: 00:00:16
Batch 52 processing...
Batch 52 completed. Time taken: 00:00:16
Batch 53 processing...
Batch 53 completed. Time taken: 00:00:16
Batch 54 processing...
Batch 54 completed. Time taken: 00:00:16
Batch 55 processing...
Batch 55 completed. Time taken: 00:00:16
Batch 56 processing...
Batch 56 completed. Time taken: 00:00:16
Batch 57 processing...
Batch 57 completed. Time taken: 00:00:16
Batch 58 processing...
Batch 58 completed. Time taken: 00:00:16
Batch 59 processing...
Batch 59 completed. Time taken: 00:00:16
Batch 60 processing...
Batch 60 completed. Time taken: 00:00:16
Batch 61 processing...
Batch 61 completed. Time taken: 00:00:16
Batch 62 processing...
Batch 62 completed. Time taken: 00:00:16
Batch 63 processing...
Batch 63 completed. Time taken: 00:00:16
Batch 64 processing...
Batch 64 completed. Time taken: 00:00:16
Batch 65 processing...
Batch 65 completed. Time taken: 00:00:17
Batch 66 processing...
Batch 66 completed. Time taken: 00:00:16
Batch 67 processing...
Batch 67 completed. Time taken: 00:00:16
Batch 68 processing...
Batch 68 completed. Time taken: 00:00:16
Batch 69 processing...
Batch 69 completed. Time taken: 00:00:16
Batch 70 processing...
Batch 70 completed. Time taken: 00:00:16
Batch 71 processing...
Batch 71 completed. Time taken: 00:00:16
Batch 72 processing...
Batch 72 completed. Time taken: 00:00:16
Batch 73 processing...
Batch 73 completed. Time taken: 00:00:16
Batch 74 processing...
Batch 74 completed. Time taken: 00:00:16
Batch 75 processing...
Batch 75 completed. Time taken: 00:00:16
Batch 76 processing...
Batch 76 completed. Time taken: 00:00:16
Batch 77 processing...
Batch 77 completed. Time taken: 00:00:16
Batch 78 processing...
Batch 78 completed. Time taken: 00:00:16
Batch 79 processing...
Batch 79 completed. Time taken: 00:00:16
Batch 80 processing...
Batch 80 completed. Time taken: 00:00:16
Batch 81 processing...
Batch 81 completed. Time taken: 00:00:16
Batch 82 processing...
Batch 82 completed. Time taken: 00:00:16
Batch 83 processing...
Batch 83 completed. Time taken: 00:00:16
Batch 84 processing...
Batch 84 completed. Time taken: 00:00:16
Batch 85 processing...
Batch 85 completed. Time taken: 00:00:16
Batch 86 processing...
Batch 86 completed. Time taken: 00:00:15
Batch 87 processing...
Batch 87 completed. Time taken: 00:00:16
Batch 88 processing...
Batch 88 completed. Time taken: 00:00:16
Batch 89 processing...
Batch 89 completed. Time taken: 00:00:16
Batch 90 processing...
Batch 90 completed. Time taken: 00:00:15
Batch 91 processing...
Batch 91 completed. Time taken: 00:00:17
Batch 92 processing...
Batch 92 completed. Time taken: 00:00:16
Batch 93 processing...
Batch 93 completed. Time taken: 00:00:16
Batch 94 processing...
Batch 94 completed. Time taken: 00:00:16
Batch 95 processing...
Batch 95 completed. Time taken: 00:00:17
Batch 96 processing...
Batch 96 completed. Time taken: 00:00:16
Batch 97 processing...
Batch 97 completed. Time taken: 00:00:16
Batch 98 processing...
Batch 98 completed. Time taken: 00:00:16
Batch 99 processing...
Batch 99 completed. Time taken: 00:00:17
Batch 100 processing...
Batch 100 completed. Time taken: 00:00:16
Batch 101 processing...
Batch 101 completed. Time taken: 00:00:16
Batch 102 processing...
Batch 102 completed. Time taken: 00:00:16
Batch 103 processing...
Batch 103 completed. Time taken: 00:00:16
Batch 104 processing...
Batch 104 completed. Time taken: 00:00:16
Batch 105 processing...
Batch 105 completed. Time taken: 00:00:16
Batch 106 processing...
Batch 106 completed. Time taken: 00:00:16
Batch 107 processing...
Batch 107 completed. Time taken: 00:00:16
Batch 108 processing...
Batch 108 completed. Time taken: 00:00:16
Batch 109 processing...
Batch 109 completed. Time taken: 00:00:16
Batch 110 processing...
Batch 110 completed. Time taken: 00:00:16
Batch 111 processing...
Batch 111 completed. Time taken: 00:00:16
Batch 112 processing...
Batch 112 completed. Time taken: 00:00:16
Batch 113 processing...
Batch 113 completed. Time taken: 00:00:16
Batch 114 processing...
Batch 114 completed. Time taken: 00:00:16
Batch 115 processing...
Batch 115 completed. Time taken: 00:00:16
Batch 116 processing...
Batch 116 completed. Time taken: 00:00:16
Batch 117 processing...
Batch 117 completed. Time taken: 00:00:16
Batch 118 processing...
Batch 118 completed. Time taken: 00:00:16
Batch 119 processing...
Batch 119 completed. Time taken: 00:00:16
Batch 120 processing...
Batch 120 completed. Time taken: 00:00:16
Batch 121 processing...
Batch 121 completed. Time taken: 00:00:16
Batch 122 processing...
Batch 122 completed. Time taken: 00:00:16
Batch 123 processing...
Batch 123 completed. Time taken: 00:00:16
Batch 124 processing...
Batch 124 completed. Time taken: 00:00:16
Batch 125 processing...
Batch 125 completed. Time taken: 00:00:16
Batch 126 processing...
Batch 126 completed. Time taken: 00:00:16
Batch 127 processing...
Batch 127 completed. Time taken: 00:00:16
Batch 128 processing...
Batch 128 completed. Time taken: 00:00:16
Batch 129 processing...
Batch 129 completed. Time taken: 00:00:16
Batch 130 processing...
Batch 130 completed. Time taken: 00:00:16
Batch 131 processing...
Batch 131 completed. Time taken: 00:00:16
Batch 132 processing...
Batch 132 completed. Time taken: 00:00:16
Batch 133 processing...
Batch 133 completed. Time taken: 00:00:16
Batch 134 processing...
Batch 134 completed. Time taken: 00:00:16
Batch 135 processing...
Batch 135 completed. Time taken: 00:00:16
Batch 136 processing...
Batch 136 completed. Time taken: 00:00:16
Batch 137 processing...
Batch 137 completed. Time taken: 00:00:16
Batch 138 processing...
Batch 138 completed. Time taken: 00:00:16
Batch 139 processing...
Batch 139 completed. Time taken: 00:00:16
Batch 140 processing...
Batch 140 completed. Time taken: 00:00:16
Batch 141 processing...
Batch 141 completed. Time taken: 00:00:16
Batch 142 processing...
Batch 142 completed. Time taken: 00:00:16
Batch 143 processing...
Batch 143 completed. Time taken: 00:00:16
Write prediction results...
transformer done!
Number of points above threshold: 115128
Total number of points: 1440000
Number of nuclei: 1115
Traceback (most recent call last):
  File "/mnt/md0/lei/projects/CellSegmentsation/CellSegmentation/minibatch/align.py", line 82, in <module>
    final_labels_flat[i] = final_labels[x, y]
IndexError: index 160000 is out of bounds for axis 0 with size 160000
align done!
loading data

calculating exp and centroids of nucleus

executing field segmentation

1 /16 batches completed! Time for batch: 8.42 seconds
2 /16 batches completed! Time for batch: 17.93 seconds
3 /16 batches completed! Time for batch: 33.97 seconds
4 /16 batches completed! Time for batch: 52.54 seconds
5 /16 batches completed! Time for batch: 79.98 seconds
